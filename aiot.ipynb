{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence of Things - Demonstration\n",
    "\n",
    "This document describes methods and mechanisms for the execution of\n",
    "Artificial Neural Networks in devices with low computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Load the Yaleface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.dataset.yalefaces as yalefaces\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 faces from 15 individuals, a total of 165 images of size (243, 320) (77760 pixels).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X, y = yalefaces.load(\"data/yalefaces\", flatten=True)\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "\n",
    "print(f\"Loaded {int(X.shape[0]/len(np.unique(y)))} faces from {len(np.unique(y))} individuals,\"\n",
    "f\" a total of {X.shape[0]} images of size {yalefaces.SHAPE} ({X[0].shape[0]} pixels).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress faces with Principal Ccomponent Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA generated 165 eigenfaces of size (243, 320).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_faces, num_pixels = X.shape\n",
    "num_principal_components = int(num_faces)\n",
    "\n",
    "pca = PCA(n_components=num_principal_components)\n",
    "pca.fit(X)\n",
    "\n",
    "print(f\"PCA generated {num_principal_components} eigenfaces of size {yalefaces.SHAPE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_compressed = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP\n",
    "### Use GPU if NVIDIA CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_compressed, X_test_compressed, y_train_compressed, y_test_compressed = train_test_split(X_compressed, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "X_train = torch.Tensor(X_train_compressed)\n",
    "y_train = torch.LongTensor(y_train_compressed)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "X_test = torch.Tensor(X_test_compressed)\n",
    "y_test = torch.LongTensor(y_test_compressed)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.mlp import MLP\n",
    "import torch.nn as nn\n",
    "\n",
    "model = MLP(in_dim=165, out_dim=15, hidden_sizes=[96, 48], activation=nn.ReLU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, device, train_loader, optimizer, loss_fn=F.cross_entropy):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # prepare\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # record\n",
    "        epoch_loss += loss.item()\n",
    "        n_samples += output.size(0)\n",
    "\n",
    "        # adjust\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss, n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.train(False)\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            scores = model.forward(x)\n",
    "            _, y_out = scores.max(1)\n",
    "            \n",
    "            num_correct += (y_out == y).sum()\n",
    "            num_samples += y_out.size(0)\n",
    "        \n",
    "        acc = float(num_correct) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_accs= []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    err, n_train_samples = train(model, device, train_data, optimizer)\n",
    "    loss = float(err)/float(n_train_samples)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # evaluate\n",
    "    acc = test(model, device, train_data)\n",
    "    train_accs.append(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fca27d76e30>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9cd146830>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.82\n"
     ]
    }
   ],
   "source": [
    "test_data = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "acc = test(model, device, test_data)\n",
    "print(f\"Test accuracy: {acc * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize model\n",
    "### Prepare model for quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our model has the following shape:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(165, 96, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(96, 48, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(48, 15, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# copy our original model\n",
    "qmodel_float = copy.deepcopy(model.net)\n",
    "qmodel_float.eval()\n",
    "\n",
    "# prepare layers\n",
    "torch.quantization.fuse_modules(qmodel_float, ['0', '1'], inplace=True)\n",
    "torch.quantization.fuse_modules(qmodel_float, ['2', '3'], inplace=True)\n",
    "\n",
    "# add quantization layers for input and output\n",
    "qmodel_float = nn.Sequential(\n",
    "    torch.quantization.QuantStub(),\n",
    "    *qmodel_float,\n",
    "    torch.quantization.DeQuantStub()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (1): LinearReLU(\n",
       "    (0): Linear(in_features=165, out_features=96, bias=False)\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): LinearReLU(\n",
       "    (0): Linear(in_features=96, out_features=48, bias=False)\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): Linear(\n",
       "    in_features=48, out_features=15, bias=False\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (6): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel_float.qconfig = torch.quantization.default_qconfig\n",
    "qmodel_float=qmodel_float.to('cpu')\n",
    "\n",
    "torch.quantization.prepare(qmodel_float, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn quantization parameters\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (x, y) in enumerate(test_data):\n",
    "        x,y = x.to('cpu'), y.to('cpu')\n",
    "        qmodel_float(x)\n",
    "\n",
    "# actually quantize weights\n",
    "qmodel = torch.quantization.convert(qmodel_float, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights size before and after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight size before quantization: 4 byte(s)\n",
      "Weight size after quantization: 1 byte(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight size before quantization:\", qmodel_float[1][0].weight.element_size(), \"byte(s)\")\n",
    "print(\"Weight size after quantization:\", qmodel[1].weight().element_size(), \"byte(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.79\n"
     ]
    }
   ],
   "source": [
    "test_data = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "acc = test(qmodel, 'cpu', test_data)\n",
    "print(f\"Test accuracy: {acc * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model sizes in KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : 86.051 KB\n",
      "Quantized: 24.900 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def print_model_size(mdl, name):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(f\"{name}: %.3f KB\" %(os.path.getsize(\"tmp.pt\")/1e3))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model, \"Original \")\n",
    "print_model_size(qmodel, \"Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating- to fixed-point conversion algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 73\n",
      "Quantized: 246\n",
      "Dequantized: 72.94234159588814\n"
     ]
    }
   ],
   "source": [
    "# scale=0.390065997838974, zero_point=59\n",
    "s = 0.390065997838974\n",
    "z = 59\n",
    "\n",
    "# to be quantized: x\n",
    "x = 73\n",
    "\n",
    "qx = round((x/s)+z)\n",
    "dqx = (qx-z)*s\n",
    "\n",
    "print(f\"Original: {x}\")\n",
    "print(f\"Quantized: {qx}\")\n",
    "print(f\"Dequantized: {dqx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read weights and quantization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params=qmodel_float.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.activation_post_process.eps', tensor([1.1921e-07])),\n",
       "             ('0.activation_post_process.min_val', tensor(-64.7021)),\n",
       "             ('0.activation_post_process.max_val', tensor(89.3256)),\n",
       "             ('1.0.weight',\n",
       "              tensor([[-8.5280e-02,  2.3521e-03, -8.3315e-02,  ...,  5.5971e-08,\n",
       "                        1.2918e-08, -1.7495e-07],\n",
       "                      [-1.0333e-01,  2.8432e-02, -3.8504e-02,  ..., -7.5666e-08,\n",
       "                        6.2811e-08,  4.6508e-07],\n",
       "                      [-3.5493e-02, -4.4640e-02,  1.0966e-02,  ..., -1.2909e-07,\n",
       "                       -2.6232e-07, -3.7944e-07],\n",
       "                      ...,\n",
       "                      [ 1.1109e-02, -4.1541e-02,  7.4070e-03,  ...,  1.2304e-07,\n",
       "                        7.9837e-08,  6.3136e-08],\n",
       "                      [ 6.4905e-02,  2.0859e-02, -1.6522e-01,  ...,  4.4794e-08,\n",
       "                        7.6903e-08, -7.7207e-08],\n",
       "                      [ 2.4909e-02, -2.6287e-02, -2.2010e-02,  ...,  6.2623e-08,\n",
       "                       -5.1053e-08,  1.7630e-07]])),\n",
       "             ('1.activation_post_process.eps', tensor([1.1921e-07])),\n",
       "             ('1.activation_post_process.min_val', tensor(0.)),\n",
       "             ('1.activation_post_process.max_val', tensor(21.6364)),\n",
       "             ('3.0.weight',\n",
       "              tensor([[-0.0128, -0.0219,  0.1425,  ..., -0.1361,  0.0163,  0.0683],\n",
       "                      [-0.0299, -0.0364, -0.0924,  ...,  0.0879,  0.1255,  0.1276],\n",
       "                      [-0.0362,  0.0116, -0.0292,  ..., -0.0438,  0.0744,  0.0612],\n",
       "                      ...,\n",
       "                      [ 0.1467,  0.0790,  0.1396,  ...,  0.0919, -0.1657, -0.0768],\n",
       "                      [-0.0429, -0.0483,  0.0313,  ..., -0.0227,  0.2069, -0.0473],\n",
       "                      [ 0.1397,  0.0571,  0.1633,  ...,  0.1132, -0.0251,  0.0426]])),\n",
       "             ('3.activation_post_process.eps', tensor([1.1921e-07])),\n",
       "             ('3.activation_post_process.min_val', tensor(0.)),\n",
       "             ('3.activation_post_process.max_val', tensor(21.1512)),\n",
       "             ('5.weight',\n",
       "              tensor([[ 0.0319,  0.1406,  0.1061,  0.0018,  0.1051, -0.1108, -0.0273, -0.1440,\n",
       "                       -0.1459, -0.1777,  0.1376, -0.1250, -0.1442,  0.0609, -0.0688,  0.1780,\n",
       "                        0.2030,  0.1527,  0.0102,  0.0791, -0.0230, -0.1236,  0.0052,  0.0706,\n",
       "                        0.0675, -0.1073, -0.0309, -0.1151, -0.1993, -0.0022, -0.1011, -0.1380,\n",
       "                       -0.1438, -0.1073, -0.1301,  0.0373, -0.1577, -0.1073, -0.1217, -0.0119,\n",
       "                       -0.0713, -0.1259, -0.1653,  0.1053, -0.0072, -0.0111, -0.1089,  0.0375],\n",
       "                      [-0.1135, -0.1098,  0.1844, -0.0312,  0.0377, -0.1472,  0.0180, -0.0836,\n",
       "                       -0.0899, -0.1698, -0.0612, -0.1996, -0.0101, -0.0637, -0.1902, -0.0044,\n",
       "                        0.0795, -0.0457,  0.0933, -0.1026, -0.0756, -0.1818, -0.0484, -0.0517,\n",
       "                       -0.0237,  0.0895,  0.0042,  0.1980, -0.0272,  0.1435,  0.1939,  0.0752,\n",
       "                       -0.1239, -0.1803,  0.1243,  0.0483, -0.0125, -0.1701, -0.0432,  0.0821,\n",
       "                       -0.1306,  0.2403, -0.0586, -0.1651,  0.1001,  0.0588, -0.1921, -0.1390],\n",
       "                      [ 0.2114,  0.0824, -0.1513,  0.1825,  0.0946,  0.1271,  0.0147, -0.0801,\n",
       "                       -0.0613, -0.2425,  0.0466,  0.0280,  0.1488,  0.0014,  0.0888,  0.0080,\n",
       "                       -0.1251, -0.1924, -0.1026,  0.2954, -0.0514,  0.2011, -0.0963, -0.0604,\n",
       "                        0.0349, -0.0526, -0.0574, -0.0143, -0.1363, -0.0655, -0.0725,  0.0748,\n",
       "                        0.0582, -0.1451, -0.2320, -0.0243, -0.0874,  0.2069, -0.0312,  0.0843,\n",
       "                       -0.1095, -0.1011, -0.0567, -0.0471, -0.0088, -0.1856, -0.1979,  0.0285],\n",
       "                      [-0.1395, -0.1308, -0.0253, -0.1828,  0.1178, -0.1126,  0.0061, -0.0113,\n",
       "                       -0.1336, -0.0710,  0.1245, -0.1843, -0.0283, -0.0444, -0.2106, -0.1353,\n",
       "                        0.1191, -0.0092, -0.1682,  0.0162, -0.0873,  0.1167, -0.0398,  0.0916,\n",
       "                        0.0440,  0.1378, -0.0729, -0.2022, -0.0286, -0.1932, -0.0238, -0.0929,\n",
       "                       -0.1161,  0.1320, -0.1669,  0.0130,  0.0562, -0.2528,  0.0564,  0.0859,\n",
       "                       -0.2173,  0.0519,  0.0906, -0.1838, -0.0226,  0.2618, -0.0503, -0.0144],\n",
       "                      [-0.1930, -0.1480,  0.1134,  0.0186, -0.2488, -0.0296, -0.0579,  0.0066,\n",
       "                       -0.1771, -0.0874, -0.0088,  0.0356, -0.0712, -0.0742, -0.2003, -0.0869,\n",
       "                        0.0625, -0.0048, -0.0227, -0.0137,  0.2014, -0.1095, -0.0207, -0.1642,\n",
       "                       -0.0448, -0.0568, -0.0274,  0.2011, -0.1852, -0.1406, -0.0222, -0.0472,\n",
       "                       -0.1527,  0.0556,  0.0948,  0.2383, -0.0686,  0.0252, -0.1394, -0.0950,\n",
       "                       -0.0514,  0.0715,  0.1404, -0.2296, -0.0217, -0.1284,  0.2795, -0.0454],\n",
       "                      [-0.1766,  0.0217, -0.1248, -0.0256, -0.0863, -0.0274, -0.0008,  0.0786,\n",
       "                        0.1547, -0.1145, -0.2536,  0.0309, -0.0790, -0.0053, -0.1658, -0.0588,\n",
       "                        0.0666, -0.0201, -0.1498,  0.1059,  0.1188, -0.1645, -0.0765, -0.0310,\n",
       "                        0.0215, -0.0418, -0.0772, -0.1078,  0.1036, -0.0122,  0.0511, -0.1041,\n",
       "                       -0.1003,  0.0422, -0.0467, -0.0318, -0.0609,  0.1088, -0.1090, -0.1355,\n",
       "                       -0.0514, -0.0971,  0.0891,  0.1501, -0.0057, -0.1559,  0.0698, -0.1554],\n",
       "                      [-0.0632, -0.0387, -0.0437, -0.0533,  0.0940,  0.1725, -0.0731,  0.0843,\n",
       "                        0.1446, -0.0259,  0.1413,  0.1301, -0.0195, -0.0030, -0.0834, -0.0322,\n",
       "                       -0.1051,  0.0865, -0.0717, -0.1241, -0.0336, -0.0027,  0.2004, -0.2292,\n",
       "                       -0.0253, -0.0648, -0.0046, -0.0443, -0.0798, -0.2237, -0.0416, -0.0413,\n",
       "                       -0.0297,  0.1410,  0.0369, -0.0963, -0.2311,  0.0299, -0.0781,  0.0205,\n",
       "                        0.1285, -0.1132, -0.2080, -0.0498,  0.0083, -0.0100, -0.0160, -0.1137],\n",
       "                      [ 0.1518, -0.1093, -0.1601, -0.1666,  0.0214, -0.1360,  0.0652,  0.0842,\n",
       "                       -0.1303,  0.0633,  0.0130,  0.0078, -0.1923,  0.1354, -0.0489,  0.0069,\n",
       "                       -0.0311, -0.0899, -0.0776, -0.1133,  0.1445, -0.0244,  0.0016, -0.1237,\n",
       "                        0.0387, -0.0883,  0.0902, -0.0893,  0.0430,  0.0274, -0.1276,  0.0549,\n",
       "                        0.1270, -0.1511,  0.0003,  0.1026, -0.3066,  0.0130,  0.1432,  0.0655,\n",
       "                        0.0623,  0.0919,  0.0289,  0.0967, -0.0406,  0.1081,  0.0478,  0.0778],\n",
       "                      [-0.0267,  0.0510, -0.1664, -0.1611, -0.2523,  0.0423,  0.0244,  0.1191,\n",
       "                        0.0092, -0.0355, -0.0605,  0.2248,  0.0014, -0.0627, -0.0666, -0.0949,\n",
       "                       -0.2664, -0.1637, -0.0337, -0.0908, -0.1944, -0.1392, -0.1089,  0.0076,\n",
       "                       -0.0060, -0.0648, -0.0267, -0.1002,  0.2557,  0.0975, -0.0430, -0.1410,\n",
       "                        0.0089, -0.0335, -0.1083, -0.0350,  0.0202,  0.1821,  0.0648, -0.1586,\n",
       "                       -0.0673, -0.1701,  0.1902, -0.2119, -0.0092, -0.0637,  0.1347,  0.1230],\n",
       "                      [-0.0539,  0.1812, -0.1018,  0.0044, -0.2113, -0.0906,  0.0095, -0.1284,\n",
       "                        0.0416, -0.0924,  0.2365, -0.1247, -0.0752,  0.0004,  0.0266, -0.0847,\n",
       "                       -0.1605, -0.0227, -0.0757, -0.1595, -0.2498, -0.0612, -0.0897,  0.2260,\n",
       "                       -0.0383, -0.0351, -0.1109, -0.1608,  0.0482, -0.1247, -0.1101, -0.1612,\n",
       "                       -0.0247, -0.1762,  0.1166, -0.1310,  0.0839, -0.0619,  0.1403, -0.1836,\n",
       "                        0.0359, -0.0992,  0.1224, -0.1040, -0.0022,  0.1808, -0.0524, -0.1756],\n",
       "                      [-0.1242, -0.1122, -0.0050, -0.1432, -0.1841, -0.0696,  0.1544, -0.1638,\n",
       "                       -0.0961, -0.0035,  0.0055, -0.0809, -0.0869, -0.0279, -0.1910, -0.1134,\n",
       "                       -0.0256, -0.0248,  0.0137, -0.0367, -0.2543,  0.1405, -0.0463, -0.1720,\n",
       "                       -0.0371,  0.0034,  0.0243, -0.0980, -0.1819, -0.1114, -0.1122,  0.1894,\n",
       "                       -0.1199,  0.1800, -0.0326, -0.0657,  0.1974,  0.0677, -0.1263,  0.1262,\n",
       "                        0.0687, -0.0548,  0.0153, -0.1981,  0.0021, -0.1691, -0.0710,  0.2251],\n",
       "                      [-0.1870, -0.1311,  0.0151,  0.1219,  0.1842, -0.0456, -0.0507, -0.0743,\n",
       "                       -0.2664, -0.2143, -0.0924, -0.1271, -0.0391,  0.0315,  0.1817, -0.1644,\n",
       "                       -0.2180, -0.0890, -0.0544, -0.1101, -0.0850, -0.2053, -0.0027, -0.1969,\n",
       "                       -0.0087,  0.1796, -0.0327,  0.1553, -0.0560,  0.0255,  0.1016, -0.0580,\n",
       "                       -0.2206,  0.0978,  0.1616, -0.1523,  0.0395, -0.1771, -0.0335,  0.0966,\n",
       "                        0.0612, -0.1407,  0.0203, -0.1089, -0.0330,  0.1172,  0.0584,  0.1044],\n",
       "                      [-0.0075, -0.0392, -0.0087, -0.1447, -0.0971, -0.1137,  0.0375, -0.0963,\n",
       "                        0.0399,  0.2216,  0.0321, -0.1403,  0.0309, -0.0467,  0.2000, -0.0701,\n",
       "                       -0.0794, -0.2004,  0.2113,  0.0229,  0.0915,  0.0431, -0.1110, -0.0438,\n",
       "                       -0.0059, -0.1210, -0.0091, -0.1098, -0.0587, -0.2183, -0.1162,  0.1318,\n",
       "                        0.1174,  0.0314, -0.0868, -0.1915,  0.1318, -0.0023,  0.1248, -0.0638,\n",
       "                       -0.0117, -0.0706, -0.1803,  0.0659, -0.0187, -0.0824, -0.0019, -0.1243],\n",
       "                      [ 0.1456,  0.0725, -0.0554, -0.0979, -0.0599, -0.1553, -0.0361,  0.2095,\n",
       "                        0.1048, -0.0402, -0.2131, -0.0234,  0.1138, -0.0059, -0.1806, -0.0021,\n",
       "                       -0.2117, -0.1511,  0.1942, -0.1220, -0.1351, -0.2090, -0.0945, -0.0054,\n",
       "                        0.0371, -0.0072, -0.1351,  0.1011, -0.2033,  0.0596, -0.0166, -0.0834,\n",
       "                       -0.1515,  0.1206, -0.1026,  0.0789, -0.0410,  0.1599, -0.0666, -0.0285,\n",
       "                       -0.0128, -0.0155, -0.1704, -0.0403, -0.0676, -0.1250,  0.1222, -0.1622],\n",
       "                      [ 0.0688, -0.2697, -0.1259, -0.1575,  0.0515, -0.0986,  0.0472, -0.1082,\n",
       "                        0.1307,  0.1004,  0.0667,  0.1680,  0.0341, -0.1788,  0.0603,  0.1277,\n",
       "                       -0.1347, -0.2112, -0.0875, -0.1049,  0.1547, -0.1488,  0.1745,  0.1825,\n",
       "                        0.0349, -0.1694, -0.1498, -0.0028, -0.2014,  0.1217, -0.0535, -0.0274,\n",
       "                       -0.0619, -0.1599,  0.1643,  0.0036, -0.1181,  0.0826, -0.1118, -0.1633,\n",
       "                       -0.2286, -0.1034,  0.0205, -0.1384,  0.0036,  0.2009, -0.1215, -0.2906]])),\n",
       "             ('5.activation_post_process.eps', tensor([1.1921e-07])),\n",
       "             ('5.activation_post_process.min_val', tensor(-18.8953)),\n",
       "             ('5.activation_post_process.max_val', tensor(16.3915))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_weights(model_state_dict, layer_idx):\n",
    "    if f'{layer_idx}.0.weight' in model_state_dict.keys():\n",
    "        weights = model_state_dict[f'{layer_idx}.0.weight'].numpy()\n",
    "    elif f'{layer_idx}.weight' in model_state_dict.keys():\n",
    "        weights = model_state_dict[f'{layer_idx}.weight'].numpy()\n",
    "    else:\n",
    "        raise KeyError(f'{layer_idx}.0.weight or {layer_idx}.weight')\n",
    "    return weights\n",
    "\n",
    "def get_layer_min(model_state_dict, layer_idx):\n",
    "    return model_state_dict[f'{layer_idx}.activation_post_process.min_val'].numpy()\n",
    "\n",
    "def get_layer_max(model_state_dict, layer_idx):\n",
    "    return model_state_dict[f'{layer_idx}.activation_post_process.max_val'].numpy()\n",
    "\n",
    "def get_scale(model_state_dict, layer_idx):\n",
    "    return model_state_dict[f'{layer_idx}.activation_post_process.max_val'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export quantized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.08484856100643383, min=0.0, max=21.636383056640625, zero=-127.0, W[0]=-0.08528020232915878\n",
      "scale=0.08294599944469976, min=0.0, max=21.151229858398438, zero=-127.0, W[0]=-0.012770934030413628\n",
      "scale=0.13837990854300705, min=-18.89534568786621, max=16.391530990600586, zero=10.0, W[0]=0.03193146362900734\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_indexes = [1, 3, 5]\n",
    "with open('mlp_params.c', 'w') as source, open('mlp_params.h', 'w') as header:\n",
    "    header.write('#ifndef MLP_PARAMS\\n#define MLP_PARAMS\\n\\n')\n",
    "    header.write('#include <stdint.h>\\n\\n\\n')\n",
    "    \n",
    "    source.write('#include \"mlp_params.h\"\\n\\n')\n",
    "    \n",
    "    for layer in layer_indexes:\n",
    "        # get raw weights and quantization parameters\n",
    "        weights = get_layer_weights(model_params, layer).flatten()\n",
    "        xmin = get_layer_min(model_params, layer)\n",
    "        xmax = get_layer_max(model_params, layer)\n",
    "        \n",
    "        header.write(f\"extern const int8_t layer_{layer}_weights[{len(weights)}];\\n\")\n",
    "        \n",
    "        # compute quantization parameters\n",
    "        num_bits = 8\n",
    "        qmin = -127\n",
    "        qmax = 128\n",
    "        eps = 1.1921e-07\n",
    "        # scale = (2 * np.maximum(np.abs(amin), amax)) / (2**num_bits)\n",
    "        # zero = amin\n",
    "\n",
    "        # FROM: https://pytorch.org/docs/stable/generated/torch.quantization.observer.MinMaxObserver.html#torch.quantization.observer.MinMaxObserver\n",
    "        scale = (xmax - xmin)/(qmax - qmin)\n",
    "        zero = qmin - np.around(xmin/scale)\n",
    "\n",
    "        # FROM: https://pytorch.org/docs/stable/_modules/torch/ao/quantization/observer.html#MinMaxObserver.calculate_qparams\n",
    "        # max_val_pos = np.amax(np.array([-xmin, xmax]))\n",
    "        # scale = max_val_pos / (float(qmax - qmin) / 2)\n",
    "        # scale = np.amax(np.array([scale, eps]))\n",
    "        # zero = np.array(0.0)\n",
    "\n",
    "        print(f\"scale={scale}, min={xmin}, max={xmax}, zero={zero}, W[0]={weights[0]}\")\n",
    "\n",
    "        # FIXME: convertion is not working properly yet...\n",
    "        \n",
    "        weights_fp = (np.around((weights / scale) + zero)).astype(int)\n",
    "\n",
    "        # weights_fp = (w - zero) * scale # this should be the reverse operation actually...\n",
    "\n",
    "        source.write(f\"const int8_t layer_{layer}_weights[{len(weights)}] = {{\")\n",
    "        for i in range(len(weights)-1):\n",
    "            source.write(f\"{weights_fp[i]}, \")\n",
    "        source.write(f\"{weights_fp[len(weights)-1]}}};\\n\\n\")\n",
    "\n",
    "    header.write('\\n#endif // end of MLP_PARAMS\\n')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with layer0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.16969712201286766, min=0.0, max=21.636383056640625, zero=0.0\n",
      "\tW[0]=-0.08528020232915878, Wq[0]=-1\n",
      "\tW[1]=0.0023521280381828547, Wq[1]=0\n",
      "\tW[2]=-0.08331505954265594, Wq[2]=0\n",
      "\tW[3]=-0.10155779868364334, Wq[3]=-1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_indexes = [1]\n",
    "with open('mlp_params.c', 'w') as source, open('mlp_params.h', 'w') as header:\n",
    "    header.write('#ifndef MLP_PARAMS\\n#define MLP_PARAMS\\n\\n')\n",
    "    header.write('#include <stdint.h>\\n\\n\\n')\n",
    "    \n",
    "    source.write('#include \"mlp_params.h\"\\n\\n')\n",
    "    \n",
    "    for layer in layer_indexes:\n",
    "        # get raw weights and quantization parameters\n",
    "        weights = get_layer_weights(model_params, layer).flatten()\n",
    "        xmin = get_layer_min(model_params, layer)\n",
    "        xmax = get_layer_max(model_params, layer)\n",
    "        \n",
    "        header.write(f\"extern const int8_t layer_{layer}_weights[{len(weights)}];\\n\")\n",
    "        \n",
    "        # compute quantization parameters\n",
    "        num_bits = 8\n",
    "        qmin = -127\n",
    "        qmax = 128\n",
    "        eps = 1.1921e-07\n",
    "        \n",
    "        \n",
    "\n",
    "        # FROM: https://pytorch.org/docs/stable/generated/torch.quantization.observer.MinMaxObserver.html#torch.quantization.observer.MinMaxObserver\n",
    "        scale = (xmax - xmin)/(qmax - qmin)\n",
    "        \n",
    "        scale = 2 * np.maximum(np.abs(xmin), xmax) / (qmax - qmin)\n",
    "        zero = 0.0\n",
    "        # zero = qmin - np.around(xmin/scale)\n",
    "\n",
    "        # FROM: https://pytorch.org/docs/stable/_modules/torch/ao/quantization/observer.html#MinMaxObserver.calculate_qparams\n",
    "        # max_val_pos = np.amax(np.array([-xmin, xmax]))\n",
    "        # scale = max_val_pos / (float(qmax - qmin) / 2)\n",
    "        # scale = np.amax(np.array([scale, eps]))\n",
    "        # zero = np.array(0.0)\n",
    "\n",
    "        # FIXME: convertion is not working properly yet...\n",
    "        weights_fp = (np.around((weights / scale) + zero)).astype(int)\n",
    "        # weights_fp = (weights - zero) * scale\n",
    "        \n",
    "        print(f\"scale={scale}, min={xmin}, max={xmax}, zero={zero}\")\n",
    "        for i in range(4):\n",
    "            print(f\"\\tW[{i}]={weights[i]}, Wq[{i}]={weights_fp[i]}\")\n",
    "\n",
    "        # weights_fp = (weights - zero) * scale # this should be the reverse operation actually...\n",
    "\n",
    "        source.write(f\"const int8_t layer_{layer}_weights[{len(weights)}] = {{\")\n",
    "        for i in range(len(weights)-1):\n",
    "            source.write(f\"{weights_fp[i]}, \")\n",
    "        source.write(f\"{weights_fp[len(weights)-1]}}};\\n\\n\")\n",
    "\n",
    "    header.write('\\n#endif // end of MLP_PARAMS\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22388823\n",
      "-0.26871186\n"
     ]
    }
   ],
   "source": [
    "layer=1\n",
    "weights = get_layer_weights(model_params, layer).flatten()\n",
    "xmin = get_layer_min(model_params, layer)\n",
    "xmax = get_layer_max(model_params, layer)\n",
    "\n",
    "print(np.amax(weights))\n",
    "print(np.amin(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.scale', tensor([1.2128])), ('0.zero_point', tensor([53])), ('1.scale', tensor(0.1704)), ('1.zero_point', tensor(0)), ('1._packed_params.dtype', torch.qint8), ('1._packed_params._packed_params', (tensor([[-0.0843,  0.0021, -0.0843,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1033,  0.0274, -0.0379,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0358, -0.0443,  0.0105,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0105, -0.0422,  0.0084,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0653,  0.0211, -0.1644,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0253, -0.0253, -0.0211,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       size=(96, 165), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0021075441036373377,\n",
      "       zero_point=0), None)), ('3.scale', tensor(0.1665)), ('3.zero_point', tensor(0)), ('3._packed_params.dtype', torch.qint8), ('3._packed_params._packed_params', (tensor([[-0.0118, -0.0216,  0.1435,  ..., -0.1356,  0.0157,  0.0688],\n",
      "        [-0.0295, -0.0373, -0.0924,  ...,  0.0884,  0.1258,  0.1278],\n",
      "        [-0.0354,  0.0118, -0.0295,  ..., -0.0432,  0.0747,  0.0609],\n",
      "        ...,\n",
      "        [ 0.1474,  0.0786,  0.1395,  ...,  0.0924, -0.1651, -0.0767],\n",
      "        [-0.0432, -0.0491,  0.0314,  ..., -0.0236,  0.2064, -0.0472],\n",
      "        [ 0.1395,  0.0570,  0.1631,  ...,  0.1140, -0.0256,  0.0432]],\n",
      "       size=(48, 96), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.001965465722605586,\n",
      "       zero_point=0), None)), ('5.scale', tensor(0.2778)), ('5.zero_point', tensor(68)), ('5._packed_params.dtype', torch.qint8), ('5._packed_params._packed_params', (tensor([[ 0.0313,  0.1395,  0.1058,  0.0024,  0.1058, -0.1106, -0.0264, -0.1443,\n",
      "         -0.1467, -0.1779,  0.1371, -0.1250, -0.1443,  0.0601, -0.0697,  0.1779,\n",
      "          0.2020,  0.1539,  0.0096,  0.0793, -0.0240, -0.1226,  0.0048,  0.0697,\n",
      "          0.0673, -0.1082, -0.0313, -0.1154, -0.1996, -0.0024, -0.1010, -0.1371,\n",
      "         -0.1443, -0.1082, -0.1298,  0.0385, -0.1587, -0.1082, -0.1226, -0.0120,\n",
      "         -0.0721, -0.1250, -0.1659,  0.1058, -0.0072, -0.0120, -0.1082,  0.0385],\n",
      "        [-0.1130, -0.1106,  0.1851, -0.0313,  0.0385, -0.1467,  0.0168, -0.0842,\n",
      "         -0.0890, -0.1707, -0.0601, -0.1996, -0.0096, -0.0625, -0.1900, -0.0048,\n",
      "          0.0793, -0.0457,  0.0938, -0.1034, -0.0745, -0.1827, -0.0481, -0.0505,\n",
      "         -0.0240,  0.0890,  0.0048,  0.1972, -0.0264,  0.1443,  0.1948,  0.0745,\n",
      "         -0.1250, -0.1803,  0.1250,  0.0481, -0.0120, -0.1707, -0.0433,  0.0818,\n",
      "         -0.1298,  0.2405, -0.0577, -0.1659,  0.1010,  0.0577, -0.1924, -0.1395],\n",
      "        [ 0.2116,  0.0818, -0.1515,  0.1827,  0.0938,  0.1274,  0.0144, -0.0793,\n",
      "         -0.0625, -0.2429,  0.0457,  0.0289,  0.1491,  0.0024,  0.0890,  0.0072,\n",
      "         -0.1250, -0.1924, -0.1034,  0.2958, -0.0505,  0.2020, -0.0962, -0.0601,\n",
      "          0.0361, -0.0529, -0.0577, -0.0144, -0.1371, -0.0649, -0.0721,  0.0745,\n",
      "          0.0577, -0.1443, -0.2308, -0.0240, -0.0866,  0.2068, -0.0313,  0.0842,\n",
      "         -0.1106, -0.1010, -0.0577, -0.0481, -0.0096, -0.1851, -0.1972,  0.0289],\n",
      "        [-0.1395, -0.1298, -0.0264, -0.1827,  0.1178, -0.1130,  0.0072, -0.0120,\n",
      "         -0.1347, -0.0721,  0.1250, -0.1851, -0.0289, -0.0433, -0.2116, -0.1347,\n",
      "          0.1202, -0.0096, -0.1683,  0.0168, -0.0866,  0.1178, -0.0409,  0.0914,\n",
      "          0.0433,  0.1371, -0.0721, -0.2020, -0.0289, -0.1924, -0.0240, -0.0938,\n",
      "         -0.1154,  0.1322, -0.1659,  0.0120,  0.0553, -0.2525,  0.0553,  0.0866,\n",
      "         -0.2164,  0.0529,  0.0914, -0.1827, -0.0216,  0.2621, -0.0505, -0.0144],\n",
      "        [-0.1924, -0.1491,  0.1130,  0.0192, -0.2477, -0.0289, -0.0577,  0.0072,\n",
      "         -0.1779, -0.0866, -0.0096,  0.0361, -0.0721, -0.0745, -0.1996, -0.0866,\n",
      "          0.0625, -0.0048, -0.0216, -0.0144,  0.2020, -0.1106, -0.0216, -0.1635,\n",
      "         -0.0457, -0.0577, -0.0264,  0.2020, -0.1851, -0.1395, -0.0216, -0.0481,\n",
      "         -0.1515,  0.0553,  0.0938,  0.2380, -0.0697,  0.0240, -0.1395, -0.0962,\n",
      "         -0.0505,  0.0721,  0.1395, -0.2308, -0.0216, -0.1274,  0.2789, -0.0457],\n",
      "        [-0.1755,  0.0216, -0.1250, -0.0264, -0.0866, -0.0264,  0.0000,  0.0793,\n",
      "          0.1539, -0.1154, -0.2525,  0.0313, -0.0793, -0.0048, -0.1659, -0.0577,\n",
      "          0.0673, -0.0192, -0.1491,  0.1058,  0.1178, -0.1635, -0.0769, -0.0313,\n",
      "          0.0216, -0.0409, -0.0769, -0.1082,  0.1034, -0.0120,  0.0505, -0.1034,\n",
      "         -0.1010,  0.0433, -0.0457, -0.0313, -0.0601,  0.1082, -0.1082, -0.1347,\n",
      "         -0.0505, -0.0962,  0.0890,  0.1491, -0.0048, -0.1563,  0.0697, -0.1563],\n",
      "        [-0.0625, -0.0385, -0.0433, -0.0529,  0.0938,  0.1731, -0.0721,  0.0842,\n",
      "          0.1443, -0.0264,  0.1419,  0.1298, -0.0192, -0.0024, -0.0842, -0.0313,\n",
      "         -0.1058,  0.0866, -0.0721, -0.1250, -0.0337, -0.0024,  0.1996, -0.2284,\n",
      "         -0.0264, -0.0649, -0.0048, -0.0433, -0.0793, -0.2236, -0.0409, -0.0409,\n",
      "         -0.0289,  0.1419,  0.0361, -0.0962, -0.2308,  0.0289, -0.0793,  0.0216,\n",
      "          0.1274, -0.1130, -0.2092, -0.0505,  0.0072, -0.0096, -0.0168, -0.1130],\n",
      "        [ 0.1515, -0.1082, -0.1611, -0.1659,  0.0216, -0.1371,  0.0649,  0.0842,\n",
      "         -0.1298,  0.0625,  0.0120,  0.0072, -0.1924,  0.1347, -0.0481,  0.0072,\n",
      "         -0.0313, -0.0890, -0.0769, -0.1130,  0.1443, -0.0240,  0.0024, -0.1226,\n",
      "          0.0385, -0.0890,  0.0890, -0.0890,  0.0433,  0.0264, -0.1274,  0.0553,\n",
      "          0.1274, -0.1515,  0.0000,  0.1034, -0.3078,  0.0120,  0.1443,  0.0649,\n",
      "          0.0625,  0.0914,  0.0289,  0.0962, -0.0409,  0.1082,  0.0481,  0.0769],\n",
      "        [-0.0264,  0.0505, -0.1659, -0.1611, -0.2525,  0.0433,  0.0240,  0.1202,\n",
      "          0.0096, -0.0361, -0.0601,  0.2236,  0.0024, -0.0625, -0.0673, -0.0938,\n",
      "         -0.2669, -0.1635, -0.0337, -0.0914, -0.1948, -0.1395, -0.1082,  0.0072,\n",
      "         -0.0048, -0.0649, -0.0264, -0.1010,  0.2549,  0.0986, -0.0433, -0.1419,\n",
      "          0.0096, -0.0337, -0.1082, -0.0361,  0.0192,  0.1827,  0.0649, -0.1587,\n",
      "         -0.0673, -0.1707,  0.1900, -0.2116, -0.0096, -0.0625,  0.1347,  0.1226],\n",
      "        [-0.0529,  0.1803, -0.1010,  0.0048, -0.2116, -0.0914,  0.0096, -0.1274,\n",
      "          0.0409, -0.0914,  0.2356, -0.1250, -0.0745,  0.0000,  0.0264, -0.0842,\n",
      "         -0.1611, -0.0216, -0.0745, -0.1587, -0.2501, -0.0601, -0.0890,  0.2260,\n",
      "         -0.0385, -0.0361, -0.1106, -0.1611,  0.0481, -0.1250, -0.1106, -0.1611,\n",
      "         -0.0240, -0.1755,  0.1178, -0.1298,  0.0842, -0.0625,  0.1395, -0.1827,\n",
      "          0.0361, -0.0986,  0.1226, -0.1034, -0.0024,  0.1803, -0.0529, -0.1755],\n",
      "        [-0.1250, -0.1130, -0.0048, -0.1443, -0.1851, -0.0697,  0.1539, -0.1635,\n",
      "         -0.0962, -0.0024,  0.0048, -0.0818, -0.0866, -0.0289, -0.1900, -0.1130,\n",
      "         -0.0264, -0.0240,  0.0144, -0.0361, -0.2549,  0.1395, -0.0457, -0.1731,\n",
      "         -0.0361,  0.0024,  0.0240, -0.0986, -0.1827, -0.1106, -0.1130,  0.1900,\n",
      "         -0.1202,  0.1803, -0.0337, -0.0649,  0.1972,  0.0673, -0.1274,  0.1250,\n",
      "          0.0697, -0.0553,  0.0144, -0.1972,  0.0024, -0.1683, -0.0721,  0.2260],\n",
      "        [-0.1876, -0.1322,  0.0144,  0.1226,  0.1851, -0.0457, -0.0505, -0.0745,\n",
      "         -0.2669, -0.2140, -0.0914, -0.1274, -0.0385,  0.0313,  0.1827, -0.1635,\n",
      "         -0.2188, -0.0890, -0.0553, -0.1106, -0.0842, -0.2044, -0.0024, -0.1972,\n",
      "         -0.0096,  0.1803, -0.0337,  0.1563, -0.0553,  0.0264,  0.1010, -0.0577,\n",
      "         -0.2212,  0.0986,  0.1611, -0.1515,  0.0385, -0.1779, -0.0337,  0.0962,\n",
      "          0.0601, -0.1419,  0.0192, -0.1082, -0.0337,  0.1178,  0.0577,  0.1034],\n",
      "        [-0.0072, -0.0385, -0.0096, -0.1443, -0.0962, -0.1130,  0.0385, -0.0962,\n",
      "          0.0409,  0.2212,  0.0313, -0.1395,  0.0313, -0.0457,  0.1996, -0.0697,\n",
      "         -0.0793, -0.1996,  0.2116,  0.0240,  0.0914,  0.0433, -0.1106, -0.0433,\n",
      "         -0.0048, -0.1202, -0.0096, -0.1106, -0.0577, -0.2188, -0.1154,  0.1322,\n",
      "          0.1178,  0.0313, -0.0866, -0.1924,  0.1322, -0.0024,  0.1250, -0.0649,\n",
      "         -0.0120, -0.0697, -0.1803,  0.0649, -0.0192, -0.0818, -0.0024, -0.1250],\n",
      "        [ 0.1467,  0.0721, -0.0553, -0.0986, -0.0601, -0.1563, -0.0361,  0.2092,\n",
      "          0.1058, -0.0409, -0.2140, -0.0240,  0.1130, -0.0048, -0.1803, -0.0024,\n",
      "         -0.2116, -0.1515,  0.1948, -0.1226, -0.1347, -0.2092, -0.0938, -0.0048,\n",
      "          0.0361, -0.0072, -0.1347,  0.1010, -0.2044,  0.0601, -0.0168, -0.0842,\n",
      "         -0.1515,  0.1202, -0.1034,  0.0793, -0.0409,  0.1587, -0.0673, -0.0289,\n",
      "         -0.0120, -0.0144, -0.1707, -0.0409, -0.0673, -0.1250,  0.1226, -0.1611],\n",
      "        [ 0.0697, -0.2693, -0.1250, -0.1563,  0.0505, -0.0986,  0.0481, -0.1082,\n",
      "          0.1298,  0.1010,  0.0673,  0.1683,  0.0337, -0.1779,  0.0601,  0.1274,\n",
      "         -0.1347, -0.2116, -0.0866, -0.1058,  0.1539, -0.1491,  0.1755,  0.1827,\n",
      "          0.0361, -0.1683, -0.1491, -0.0024, -0.2020,  0.1226, -0.0529, -0.0264,\n",
      "         -0.0625, -0.1611,  0.1635,  0.0024, -0.1178,  0.0818, -0.1106, -0.1635,\n",
      "         -0.2284, -0.1034,  0.0216, -0.1395,  0.0024,  0.2020, -0.1226, -0.2909]],\n",
      "       size=(15, 48), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.002404525177553296,\n",
      "       zero_point=0), None))])\n"
     ]
    }
   ],
   "source": [
    "qmodel_params = qmodel.state_dict()\n",
    "print(qmodel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021075441036373377\n"
     ]
    }
   ],
   "source": [
    "layer_idx = 1\n",
    "t = qmodel_params[f'{layer_idx}._packed_params._packed_params']\n",
    "print(t[0][0].q_scale())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1.activation_post_process.max_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m xmin \u001b[39m=\u001b[39m get_layer_min(model_params, layer)\n\u001b[1;32m     14\u001b[0m xmax \u001b[39m=\u001b[39m get_layer_max(model_params, layer)\n\u001b[0;32m---> 15\u001b[0m scale \u001b[39m=\u001b[39m get_scale(qmodel_params, layer)\n\u001b[1;32m     17\u001b[0m header\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mextern const int8_t layer_\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39m_weights[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(weights)\u001b[39m}\u001b[39;00m\u001b[39m];\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# compute quantization parameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [24], line 17\u001b[0m, in \u001b[0;36mget_scale\u001b[0;34m(model_state_dict, layer_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_scale\u001b[39m(model_state_dict, layer_idx):\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m model_state_dict[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mlayer_idx\u001b[39m}\u001b[39;49;00m\u001b[39m.activation_post_process.max_val\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyError\u001b[0m: '1.activation_post_process.max_val'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_indexes = [1]\n",
    "with open('mlp_params.c', 'w') as source, open('mlp_params.h', 'w') as header:\n",
    "    header.write('#ifndef MLP_PARAMS\\n#define MLP_PARAMS\\n\\n')\n",
    "    header.write('#include <stdint.h>\\n\\n\\n')\n",
    "    \n",
    "    source.write('#include \"mlp_params.h\"\\n\\n')\n",
    "    \n",
    "    for layer in layer_indexes:\n",
    "        # get raw weights and quantization parameters\n",
    "        weights = get_layer_weights(model_params, layer).flatten()\n",
    "        xmin = get_layer_min(model_params, layer)\n",
    "        xmax = get_layer_max(model_params, layer)\n",
    "        scale = get_scale(qmodel_params, layer) # FIXME implement get_scale using the cell code above\n",
    "        \n",
    "        header.write(f\"extern const int8_t layer_{layer}_weights[{len(weights)}];\\n\")\n",
    "        \n",
    "        # compute quantization parameters\n",
    "        num_bits = 8\n",
    "        qmin = -127\n",
    "        qmax = 128\n",
    "        eps = 1.1921e-07\n",
    "        \n",
    "        \n",
    "\n",
    "        # FROM: https://pytorch.org/docs/stable/generated/torch.quantization.observer.MinMaxObserver.html#torch.quantization.observer.MinMaxObserver\n",
    "        scale = (xmax - xmin)/(qmax - qmin)\n",
    "        \n",
    "        scale = 2 * np.maximum(np.abs(xmin), xmax) / (qmax - qmin)\n",
    "        zero = 0.0\n",
    "        # zero = qmin - np.around(xmin/scale)\n",
    "\n",
    "        # FROM: https://pytorch.org/docs/stable/_modules/torch/ao/quantization/observer.html#MinMaxObserver.calculate_qparams\n",
    "        # max_val_pos = np.amax(np.array([-xmin, xmax]))\n",
    "        # scale = max_val_pos / (float(qmax - qmin) / 2)\n",
    "        # scale = np.amax(np.array([scale, eps]))\n",
    "        # zero = np.array(0.0)\n",
    "\n",
    "        # FIXME: convertion is not working properly yet...\n",
    "        weights_fp = (np.around((weights / scale) + zero)).astype(int)\n",
    "        # weights_fp = (weights - zero) * scale\n",
    "        \n",
    "        print(f\"scale={scale}, min={xmin}, max={xmax}, zero={zero}\")\n",
    "        for i in range(4):\n",
    "            print(f\"\\tW[{i}]={weights[i]}, Wq[{i}]={weights_fp[i]}\")\n",
    "\n",
    "        # weights_fp = (weights - zero) * scale # this should be the reverse operation actually...\n",
    "\n",
    "        source.write(f\"const int8_t layer_{layer}_weights[{len(weights)}] = {{\")\n",
    "        for i in range(len(weights)-1):\n",
    "            source.write(f\"{weights_fp[i]}, \")\n",
    "        source.write(f\"{weights_fp[len(weights)-1]}}};\\n\\n\")\n",
    "\n",
    "    header.write('\\n#endif // end of MLP_PARAMS\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d49e41cedb4d2b8ded917810122a7ef9cd3b4ce1bdb33db87dd80104225a604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
